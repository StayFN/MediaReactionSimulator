{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5ab909-3656-4154-8bf9-8b5d86da078c",
   "metadata": {},
   "source": [
    "# Describe YouTube Shorts with HuggingChat LLM\n",
    "\n",
    "This script combines all previous Jupyter notebooks into one program which includes all necessary functionality to extract YouTube shorts by search terms and generate LLM descriptions.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Set up authentication. See [Authentication](#authentication).\n",
    "2. Specify the search terms you want to use to extract the YouTube Shorts IDs in the file `csv/input/youtube_shorts_search_terms.csv`.\n",
    "3. Run the program by executing all cells. The last cell will call the `main()` function which will trigger the program execution. The `main()` function accepts an integer as parameter which specifies the max. results per search term.\n",
    "4. Output is saved in `csv/output/youtube_shorts_with_chatbot_summary.csv`.\n",
    "\n",
    "## <a id=\"authentication\"></a> Authentication:\n",
    "\n",
    "Edit `client_secrets.json` to contain all relevant authentication information in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"api_key\":\"<YOUR_YOUTUBE_API_KEY>\", \n",
    "    \"huggingLogin\":\"<YOUR_EMAIL>\",\n",
    "    \"huggingPassword\":\"<YOUR_PASSWORD>\"\n",
    "}\n",
    "```\n",
    "\n",
    "Create your YouTube API Key here: <https://console.cloud.google.com/apis/credentials>\n",
    "\n",
    "Create your HuggingChat credentials here: <https://huggingface.co/chat/>\n",
    "\n",
    "## CSV File Descriptions:\n",
    "\n",
    "`SHORT_ID_SEARCH_CSV`: CSV file which contains a list with all search queries. Before use, provide your search terms in this file.\n",
    "\n",
    "`USED_SEARCH_QUERIES`: CSV file which contains all already used search queries. You don't need to touch this file.\n",
    "\n",
    "`NEW_SHORT_IDS`: CSV file which contains the new short IDs that will be processed by during the Shorts extraction phase of the program. You don't need to touch this file.\n",
    "\n",
    "`YOUTUBE_SHORTS_INFO`: CSV file which contains the extracted YouTube Shorts information. You don't need to touch this file.\n",
    "\n",
    "`YOUTUBE_SHORTS_WITH_CHATBOT_SUMMARY`: CSV file which contains the final output. Your results will be merged with the previous results. You don't need to edit this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e48e790-5825-49fd-934f-e181767a5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "SHORT_ID_SEARCH_CSV = 'csv/input/youtube_shorts_search_terms.csv'\n",
    "USED_SEARCH_QUERIES = 'csv/tmp/used_search_queries.csv'\n",
    "NEW_SHORT_IDS = 'csv/tmp/new_short_ids_temp.csv'\n",
    "YOUTUBE_SHORTS_INFO = \"csv/tmp/youtube_shorts_description.csv\"\n",
    "YOUTUBE_SHORTS_WITH_CHATBOT_SUMMARY = 'csv/output/youtube_shorts_with_chatbot_summary.csv'\n",
    "\n",
    "API_VERSION = \"v3\"\n",
    "API_SERVICE_NAME = \"youtube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f016b8df-cfdc-477d-bd27-d5f128a1dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from hugchat import hugchat\n",
    "from hugchat.login import Login\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf424aa-3d46-4079-a4b0-3f2978407633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "with open('client_secrets.json', 'r') as file:\n",
    "    secrets = json.load(file)\n",
    "\n",
    "# Set up the API key and YouTube API client\n",
    "api_key = secrets['api_key']  \n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "login = secrets['huggingLogin']\n",
    "password = secrets['huggingPassword']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc09041-5a83-4977-90c1-ec9243f8d44b",
   "metadata": {},
   "source": [
    "### Get Short IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12cb0961-60d5-4856-8664-0fd35e5578f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shorts\n",
    "def run_query(query, max_results):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    # Add a keyword to the query to search specifically for Shorts\n",
    "    modified_query = query + \" #shorts\"\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        # Fetch search results\n",
    "        request = youtube.search().list(\n",
    "            part=\"id\",\n",
    "            q=modified_query,\n",
    "            type=\"video\",\n",
    "            maxResults=50,  # Adjust as needed (max 50 per request)\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract video IDs\n",
    "        for item in response.get('items', []):\n",
    "            video_ids.append(item['id']['videoId'])\n",
    "            if len(video_ids) >= max_results:\n",
    "                break\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'Query': [query], 'Video_IDs': [video_ids]})\n",
    "    return df\n",
    "\n",
    "def get_short_ids(max_results_per_search_term):\n",
    "    search_terms_df = pd.read_csv(SHORT_ID_SEARCH_CSV)\n",
    "    combined_results_df = pd.DataFrame()\n",
    "    for search_term in tqdm(search_terms_df['Search Terms'], desc=\"[+] Processing Search Terms\"):\n",
    "        result_df = run_query(search_term, max_results_per_search_term)\n",
    "    \n",
    "        # Combine the result with the combined DataFrame\n",
    "        combined_results_df = pd.concat([combined_results_df, result_df], ignore_index=True)\n",
    "\n",
    "    # only merge video ids that have not already been scanned\n",
    "    curr_video_ids_list = get_video_list(USED_SEARCH_QUERIES)\n",
    "    clean_result_df = pd.DataFrame()\n",
    "\n",
    "    for index, row in combined_results_df.iterrows():\n",
    "        clean_list = []\n",
    "        for id in row['Video_IDs']:\n",
    "            if id not in curr_video_ids_list:\n",
    "                clean_list.append(id)\n",
    "        clean_result_df = pd.concat([clean_result_df, pd.DataFrame({'Query': [row['Query']],'Video_IDs': [clean_list]})], ignore_index=True)\n",
    "\n",
    "    curr_video_ids = pd.read_csv(USED_SEARCH_QUERIES)    \n",
    "    df_merged = pd.concat([curr_video_ids, clean_result_df], ignore_index=True)\n",
    "\n",
    "    # Save to CSV file\n",
    "    df_merged.to_csv(USED_SEARCH_QUERIES, index=False)\n",
    "    clean_result_df.to_csv(NEW_SHORT_IDS, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3c8a6-a546-42ad-b68a-8957b07bb85e",
   "metadata": {},
   "source": [
    "### Extract Short Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0930946a-f14c-44bc-90e9-03c3de17a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YTVideo:\n",
    "    def __init__(self, videoId=\"\"):\n",
    "        self.youtubeClient = self.getYoutubeAPICLient()\n",
    "        \n",
    "        self.videoId = videoId\n",
    "        self.transcript, self.duration = self.extractTranscript()\n",
    "        if self.transcript is None and self.duration is None:\n",
    "            return None\n",
    "       \n",
    "        videoInfo = self.getVideoInfo()\n",
    "        self.title = videoInfo['snippet']['title'].encode('utf-8', errors='replace')\n",
    "        self.description = videoInfo['snippet']['description'].encode('utf-8', errors='replace')\n",
    "        self.channelTitle = videoInfo['snippet']['channelTitle'].encode('utf-8', errors='replace')\n",
    "        self.publishedAt = videoInfo['snippet']['publishedAt']\n",
    "        self.views = videoInfo['statistics']['viewCount']\n",
    "\n",
    "        if 'likeCount' in videoInfo['statistics']:\n",
    "            self.likes = videoInfo['statistics']['likeCount']\n",
    "        else:\n",
    "            self.likes = 0\n",
    "\n",
    "        if 'commentCount' in videoInfo['statistics']:\n",
    "            self.commentCount = videoInfo['statistics']['commentCount']\n",
    "            try:\n",
    "                self.top10comments = self.getTopComments()\n",
    "            except:\n",
    "                self.top10comments = []\n",
    "        else:\n",
    "            self.commentCount = 0\n",
    "            self.top10comments = []\n",
    "        \n",
    "        self.category = self.getCategoryByID(videoInfo['snippet']['categoryId'])\n",
    "        return\n",
    "\n",
    "    def getYoutubeAPICLient(self):\n",
    "        return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, developerKey=api_key)\n",
    "    \n",
    "    # extract transcript and transform it to string\n",
    "    def extractTranscript(self):\n",
    "        transcript = \"\"\n",
    "        duration = 0\n",
    "        try:\n",
    "            transcriptList = YouTubeTranscriptApi.get_transcript(self.videoId)\n",
    "        except:\n",
    "            return None,None\n",
    "        for t in transcriptList:\n",
    "            transcript += f\"{t['text']} \"\n",
    "            duration += t['duration']\n",
    "        return transcript, duration\n",
    "\n",
    "    # extract video info from YT API\n",
    "    def getVideoInfo(self):\n",
    "        response = self.youtubeClient.videos().list(part=\"snippet,contentDetails,statistics\", id=self.videoId).execute()\n",
    "        return response['items'][0]\n",
    "        \n",
    "    def getCategoryByID(self, categoryID):\n",
    "        response = self.youtubeClient.videoCategories().list(part=\"snippet\", id=categoryID).execute()\n",
    "        return response['items'][0]['snippet']['title'].encode('utf-8', errors='replace')\n",
    "\n",
    "    def getTopComments(self):\n",
    "        response = self.youtubeClient.commentThreads().list(part=\"snippet\", order=\"relevance\", maxResults=10, videoId=self.videoId).execute()\n",
    "        comment_list = []\n",
    "        for comment in response['items']:\n",
    "            comment_list.append(comment['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "        return comment_list\n",
    "\n",
    "# receives a list of video IDs and generates a csv file with available information about the video\n",
    "def generate_csv():\n",
    "    vidList = get_video_list(NEW_SHORT_IDS)\n",
    "    # open csv and create csv writer\n",
    "    with open(YOUTUBE_SHORTS_INFO, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        # extract info from YT API and write to csv file\n",
    "        writer.writerow([\"Video ID\", \"Video Title\", \"Channel Title\", \"Transcript\", \"Duration\", \"Words per Second\", \"Number of Comments\", \"Top10 Comments\", \"Category\", \"Views\", \"Likes\"])\n",
    "        for vidID in tqdm(vidList, desc=\"[+] Extracting Information from YouTube API\"):\n",
    "            video = YTVideo(vidID)\n",
    "            if video.transcript is None:\n",
    "                continue\n",
    "            writer.writerow([video.videoId, video.title, video.channelTitle, video.transcript, video.duration, len(video.transcript.split(\" \"))/video.duration, video.commentCount, video.top10comments, video.category, video.views, video.likes])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d66530-d350-4cb0-be19-2d34cca85f4e",
   "metadata": {},
   "source": [
    "### HuggingChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c37d0f6-1ed7-489d-a117-36de151cad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_descriptions():\n",
    "    sign = Login(login, password)\n",
    "    cookies = sign.login()\n",
    "\n",
    "    # Save cookies to the local directory\n",
    "    cookie_path_dir = \"./cookies_snapshot\"\n",
    "    sign.saveCookiesToDir(cookie_path_dir)\n",
    "\n",
    "    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())  # or cookie_path=\"usercookies/<email>.json\"\n",
    "\n",
    "    # load shorts information\n",
    "    df_shorts = pd.read_csv(YOUTUBE_SHORTS_INFO, sep=\";\")\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df_shorts['Custom Query'] = df_shorts.apply(create_custom_query, axis=1)\n",
    "\n",
    "    # Process the DataFrame in batches of 20 and write to the CSV file\n",
    "    process_in_batches(df_shorts, 10, YOUTUBE_SHORTS_WITH_CHATBOT_SUMMARY, chatbot)\n",
    "    \n",
    "    return\n",
    "\n",
    "def create_custom_query(row):\n",
    "    return (\n",
    "        \"You are a copywriter, create a 100 word summary of what this Youtube Short is about. \"\n",
    "        \"Provide a neutral description. The summary should describe the overall atmosphere \"\n",
    "        \"and pace of the video. It should also highlight important events from the video. \"\n",
    "        \"Do not include any statements about a viewers response to the content or the overall \"\n",
    "        \"viewing experience. Output the raw summary text.\\n\"\n",
    "        \"Title: {}\\n\"\n",
    "        \"Channel: {}\\n\"\n",
    "        \"Transcript: {}\\n\"\n",
    "        \"Comments: {}\\n\"\n",
    "        \"Category: {}\\n\"\n",
    "    ).format(\n",
    "        row['Video Title'], \n",
    "        row['Channel Title'], \n",
    "        row['Transcript'], \n",
    "        row['Top10 Comments'], \n",
    "        row['Category']\n",
    "    )\n",
    "\n",
    "def get_chatbot_summary(row, chatbot):\n",
    "    #Cast to String for regex, since query returns Message object\n",
    "    time.sleep(10)\n",
    "    return str(chatbot.query(row['Custom Query']))\n",
    "\n",
    "# Process the DataFrame in batches and update the DataFrame\n",
    "def process_in_batches(dataframe, batch_size, output_csv, chatbot):\n",
    "    for start in range(0, len(dataframe), batch_size):\n",
    "        end = min(start + batch_size, len(dataframe))\n",
    "        batch = dataframe.iloc[start:end]\n",
    "\n",
    "        with tqdm(total=len(batch), desc=f\"Processing Batch {start}-{end}\") as pbar:\n",
    "            try:\n",
    "                # Process each row and update the DataFrame\n",
    "                for i, row in batch.iterrows():\n",
    "                    dataframe.at[i, 'LLM Summary'] = get_chatbot_summary(row, chatbot)\n",
    "                    pbar.update(1)  # Update the batch progress bar\n",
    "\n",
    "                # Append the current state of the DataFrame to the CSV file\n",
    "                old_df = pd.read_csv(output_csv)\n",
    "                new_df = pd.concat([old_df, dataframe], ignore_index=True)\n",
    "                new_df.to_csv(output_csv, index=False)\n",
    "\n",
    "                print(f\"Batch {start} to {end} processed successfully\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {start} to {end}: {e}\")\n",
    "                time.sleep(60)  # Sleep timer for rate limiting\n",
    "                process_in_batches(dataframe.iloc[start:], batch_size, output_csv)  # Restart from the current batch\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c37446-9b16-49a0-b732-2cef6169c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_video_list(filename):\n",
    "    ret_list = []\n",
    "    df = pd.read_csv(filename)\n",
    "    df.Video_IDs = df.Video_IDs.apply(literal_eval)\n",
    "    for row in df.Video_IDs:\n",
    "        ret_list += row\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1a4043-b43d-4a24-83af-60a7a4457ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_results_per_search_term=50):\n",
    "    # get youtube short IDs\n",
    "    print('1. STEP: Getting YouTube Short IDs')\n",
    "    get_short_ids(max_results_per_search_term)\n",
    "\n",
    "    # extract short information\n",
    "    print('2. STEP: Extract Youtube Short Info')\n",
    "    generate_csv()\n",
    "\n",
    "    # generate LLM descriptions\n",
    "    print('3. STEP: Generate LLM Descriptions')\n",
    "    get_llm_descriptions()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c18b16f-aeba-4f19-a4a6-d1e17930423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. STEP: Getting YouTube Short IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+] Processing Search Terms: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. STEP: Extract Youtube Short Info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+] Extracting Information from YouTube API: 100%|█████████████████████████████████████| 20/20 [00:21<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. STEP: Generate LLM Descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 0-10: 100%|███████████████████████████████████████████████████████████| 10/10 [03:29<00:00, 20.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 10 processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 10-12: 100%|████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 to 12 processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d119c-f451-42ba-a87a-4b390b660e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
